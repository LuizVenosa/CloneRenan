{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9376c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from networkx.readwrite import json_graph\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_lg\")\n",
    "except:\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"pt_core_news_lg\"])\n",
    "    nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "# Blacklist sempre em UPPER para comparação segura\n",
    "TERMOS_BANIDOS = {\n",
    "    'LIKE LIVE', 'VAMOS VAMOS', 'LIKE NA LIVE', 'INSCREVA CANAL', \n",
    "    'BOA NOITE', 'VALET PLUS', 'BANCO MASTER', 'RENAN SANTOS', \n",
    "    'PESSOAL', 'GENTE', 'COISA', 'AQUI', 'TÁ', 'NÉ', 'RENAN'\n",
    "}\n",
    "\n",
    "def extrair_temas_naturais(textos, top_n=20):\n",
    "    from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "    all_stops = STOP_WORDS.union({'então', 'falar', 'querer', 'ficar', 'olha', 'hum', 'hm'})\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(2, 3),\n",
    "        stop_words=list(all_stops),\n",
    "        max_features=200,\n",
    "        min_df=3\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(textos)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    sums = tfidf_matrix.sum(axis=0)\n",
    "    \n",
    "    ranking_contagem = Counter()\n",
    "    for col, term in enumerate(feature_names):\n",
    "        termo_upper = term.strip().upper()\n",
    "        if termo_upper not in TERMOS_BANIDOS:\n",
    "            ranking_contagem[termo_upper] += sums[0, col]\n",
    "            \n",
    "    # Retornamos em Title Case (Estilo Normal)\n",
    "    return [termo.title() for termo, score in ranking_contagem.most_common(top_n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "contagem_geral = Counter()\n",
    "co_occurrences = Counter()\n",
    "pasta_srt = \n",
    "caminhos = [os.path.join(pasta_srt, f) for f in os.listdir(pasta_srt) if f.endswith(\".srt\")]\n",
    "\n",
    "for path in caminhos:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        texto = \" \".join([l.strip() for l in f.readlines() if not l.strip().isdigit() and \"-->\" not in l])\n",
    "        all_texts.append(texto)\n",
    "\n",
    "temas_descobertos = extrair_temas_naturais(all_texts)\n",
    "\n",
    "for texto in all_texts:\n",
    "    chunks = [texto[i:i+600] for i in range(0, len(texto), 600)]\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        \n",
    "        # 1. Unificar tudo para UPPER para o cálculo (Evita duplicatas)\n",
    "        ents = [ent.text.strip().upper() for ent in doc.ents if ent.label_ in ['PER', 'ORG']]\n",
    "        temas = [t.upper() for t in temas_descobertos if t.lower() in chunk.lower()]\n",
    "        \n",
    "        todos_nos_upper = list(set(ents + temas))\n",
    "        \n",
    "        # Filtrar banidos\n",
    "        todos_nos_upper = [n for n in todos_nos_upper if n not in TERMOS_BANIDOS and len(n) > 2]\n",
    "        \n",
    "        for no in todos_nos_upper: \n",
    "            contagem_geral[no] += 1\n",
    "        \n",
    "        if len(todos_nos_upper) > 1:\n",
    "            for combo in combinations(sorted(todos_nos_upper), 2):\n",
    "                co_occurrences[combo] += 1\n",
    "\n",
    "# 2. Criar o Grafo e converter os nomes para Title Case na hora de adicionar os nós\n",
    "G = nx.Graph()\n",
    "for (node1, node2), weight in co_occurrences.items():\n",
    "    if contagem_geral[node1] >= 5 and contagem_geral[node2] >= 5:\n",
    "        # Converte para estilo normal (Ex: \"Lula\" em vez de \"LULA\")\n",
    "        n1_normal = node1.title()\n",
    "        n2_normal = node2.title()\n",
    "        \n",
    "        G.add_edge(n1_normal, n2_normal, weight=weight)\n",
    "        G.nodes[n1_normal]['mencoes'] = contagem_geral[node1]\n",
    "        G.nodes[n2_normal]['mencoes'] = contagem_geral[node2]\n",
    "        \n",
    "# 3. Formatar o ranking para estilo normal para o frontend\n",
    "ranking_normalizado = [(nome.title(), qtd) for nome, qtd in contagem_geral.most_common(30)]\n",
    "        \n",
    "resultado = {\n",
    "    \"stats\": {\n",
    "        \"total_arquivos\": len(caminhos),\n",
    "        \"ranking\": ranking_normalizado, \n",
    "        \"temas_descobertos\": [t.title() for t in temas_descobertos] \n",
    "    },\n",
    "    \"grafo\": json_graph.node_link_data(G, edges=\"edges\")\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CloneRenan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
